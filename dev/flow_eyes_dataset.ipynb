{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fytroo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import BatchNormalization, Flatten, Dense\n",
    "from keras.layers import Input, Conv2D, Convolution2D\n",
    "from keras.layers import Activation, concatenate, Dropout\n",
    "from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "import Augmentor\n",
    "import json\n",
    "\n",
    "import tuner\n",
    "from tuner import utils\n",
    "from tuner import load_data\n",
    "from tuner import augment_data\n",
    "from tuner import use_hyperas\n",
    "from tuner import net\n",
    "from tuner.dataset import ClassificationDataset, AugmentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "params = EasyDict({\n",
    "    'src_dir': '../micin-dataset/eyes',\n",
    "    'dst_dir': './eyes'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert  dataset from brain-dir to ready-dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = load_data.df_fromdir_eyes(params.src_dir, teaching_file='label.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_data.classed_dir_fromdf(df, \"ready/eyes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset via ready-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eyes_dataset = ClassificationDataset('ready/eyes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'異常あり': 208, '異常なし': 517}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eyes_dataset.counts_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 725 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/train/output."
     ]
    }
   ],
   "source": [
    "aug_eyes = AugmentDataset(eyes_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search best condition of data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def data():\n",
      "    train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/train' \n",
      "    test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/validation'\n",
      "    resize = 96 \n",
      "    rescale = 1 \n",
      "    df = load_data.df_fromdir_classed(train_dir)\n",
      "    x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "    df = load_data.df_fromdir_classed(test_dir)\n",
      "    x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "\n",
      "    y_train = to_categorical(y_train)\n",
      "    y_test = to_categorical(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test\n",
      "\n",
      "            name label             handle  \\\n",
      "0   113_0131.JPG  異常なし  異常なし/113_0131.JPG   \n",
      "1   113_0007.JPG  異常なし  異常なし/113_0007.JPG   \n",
      "2   148_0085.JPG  異常なし  異常なし/148_0085.JPG   \n",
      "3   143_0103.JPG  異常なし  異常なし/143_0103.JPG   \n",
      "4   148_0014.JPG  異常なし  異常なし/148_0014.JPG   \n",
      "5   113_0096.JPG  異常なし  異常なし/113_0096.JPG   \n",
      "6   184_0171.JPG  異常なし  異常なし/184_0171.JPG   \n",
      "7   105_0045.JPG  異常なし  異常なし/105_0045.JPG   \n",
      "8   179_0051.JPG  異常なし  異常なし/179_0051.JPG   \n",
      "9   175_0025.JPG  異常なし  異常なし/175_0025.JPG   \n",
      "10  143_0167.JPG  異常なし  異常なし/143_0167.JPG   \n",
      "11  179_0033.JPG  異常なし  異常なし/179_0033.JPG   \n",
      "12  148_0024.JPG  異常なし  異常なし/148_0024.JPG   \n",
      "13  185_0103.JPG  異常なし  異常なし/185_0103.JPG   \n",
      "14  198_0024.JPG  異常なし  異常なし/198_0024.JPG   \n",
      "15  113_0112.JPG  異常なし  異常なし/113_0112.JPG   \n",
      "16  182_0053.JPG  異常なし  異常なし/182_0053.JPG   \n",
      "17  197_0114.JPG  異常なし  異常なし/197_0114.JPG   \n",
      "18  129_0078.JPG  異常なし  異常なし/129_0078.JPG   \n",
      "19  159_0122.JPG  異常なし  異常なし/159_0122.JPG   \n",
      "20  105_0077.JPG  異常なし  異常なし/105_0077.JPG   \n",
      "21  108_0131.JPG  異常なし  異常なし/108_0131.JPG   \n",
      "22  185_0170.JPG  異常なし  異常なし/185_0170.JPG   \n",
      "23  178_0033.JPG  異常あり  異常あり/178_0033.JPG   \n",
      "24  173_0043.JPG  異常あり  異常あり/173_0043.JPG   \n",
      "25  179_0106.JPG  異常あり  異常あり/179_0106.JPG   \n",
      "26  184_0216.JPG  異常あり  異常あり/184_0216.JPG   \n",
      "27  168_0113.JPG  異常あり  異常あり/168_0113.JPG   \n",
      "28  108_0026.JPG  異常あり  異常あり/108_0026.JPG   \n",
      "29  105_0019.JPG  異常あり  異常あり/105_0019.JPG   \n",
      "30  143_0058.JPG  異常あり  異常あり/143_0058.JPG   \n",
      "31  108_0162.JPG  異常あり  異常あり/108_0162.JPG   \n",
      "32  127_0135.JPG  異常あり  異常あり/127_0135.JPG   \n",
      "33  178_0020.JPG  異常あり  異常あり/178_0020.JPG   \n",
      "34  168_0009.JPG  異常あり  異常あり/168_0009.JPG   \n",
      "35  175_0109.JPG  異常あり  異常あり/175_0109.JPG   \n",
      "36  127_0162.JPG  異常あり  異常あり/127_0162.JPG   \n",
      "37  143_0082.JPG  異常あり  異常あり/143_0082.JPG   \n",
      "38  171_0041.JPG  異常あり  異常あり/171_0041.JPG   \n",
      "39  171_0023.JPG  異常あり  異常あり/171_0023.JPG   \n",
      "40  178_0158.JPG  異常あり  異常あり/178_0158.JPG   \n",
      "41  171_0102.JPG  異常あり  異常あり/171_0102.JPG   \n",
      "42  129_0108.JPG  異常あり  異常あり/129_0108.JPG   \n",
      "43  168_0122.JPG  異常あり  異常あり/168_0122.JPG   \n",
      "44  129_0050.JPG  異常あり  異常あり/129_0050.JPG   \n",
      "45  129_0031.JPG  異常あり  異常あり/129_0031.JPG   \n",
      "\n",
      "                                                 path  \n",
      "0   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "1   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "2   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "3   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "4   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "5   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "6   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "7   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "8   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "9   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "10  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "11  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "12  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "13  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "14  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "15  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "16  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "17  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "18  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "19  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "20  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "21  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "22  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "23  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "24  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "25  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "26  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "27  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "28  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "29  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "30  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "31  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "32  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "33  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "34  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "35  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "36  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "37  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "38  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "39  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "40  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "41  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "42  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "43  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "44  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "45  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import subprocess\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import hp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import BatchNormalization, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Conv2D, Convolution2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, concatenate, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils, to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import Augmentor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import load_data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import net\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas_data import data\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'conditional': hp.choice('conditional', [True, False]),\n",
      "        'conditional_1': hp.choice('conditional_1', [True, False]),\n",
      "        'conditional_2': hp.choice('conditional_2', [True, False]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/train' \n",
      "  3: test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/validation'\n",
      "  4: resize = 96 \n",
      "  5: rescale = 1 \n",
      "  6: df = load_data.df_fromdir_classed(train_dir)\n",
      "  7: x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "  8: df = load_data.df_fromdir_classed(test_dir)\n",
      "  9: x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      " 10: \n",
      " 11: y_train = to_categorical(y_train)\n",
      " 12: y_test = to_categorical(y_test)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     params = eval(open('tmp_params.json', 'r').read())\n",
      "   4: \n",
      "   5:     n_out = params['n_out']\n",
      "   6:     input_shape = tuple(params['input_shape'])\n",
      "   7:     batch_size = params['batch_size']\n",
      "   8:     epochs = params['epochs']\n",
      "   9:     lossfun = params['lossfun']\n",
      "  10:     optimizer = params['optimizer']\n",
      "  11:     metrics = ['accuracy']\n",
      "  12: \n",
      "  13:     steps_per_epoch = len(x_train) // batch_size\n",
      "  14: \n",
      "  15:     p = Augmentor.Pipeline()\n",
      "  16: \n",
      "  17:     p.flip_left_right(probability=0.5)\n",
      "  18:     if conditional(space['conditional']):\n",
      "  19:         p.crop_random(probability=1, percentage_area=0.8)\n",
      "  20:         p.resize(probability=1, width=96, height=96)\n",
      "  21:     if conditional(space['conditional_1']):\n",
      "  22:         p.random_erasing(probability=0.5, rectangle_area=0.2)\n",
      "  23:     if conditional(space['conditional_2']):\n",
      "  24:         p.shear(probability=0.3, max_shear_left=2, max_shear_right=2)\n",
      "  25:     print('-' * 80)\n",
      "  26:     p.status()\n",
      "  27:     g = p.keras_generator_from_array(x_train, y_train, batch_size=batch_size)\n",
      "  28:     g = ((x / 255., y) for (x, y) in g)\n",
      "  29: \n",
      "  30:     inputs = Input(shape=input_shape)\n",
      "  31:     x = inputs\n",
      "  32:     x = Conv2D(32, (3, 3))(x)\n",
      "  33:     x = Conv2D(32, (3, 3))(x)\n",
      "  34:     x = Activation('relu')(x)\n",
      "  35:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  36:     x = Conv2D(64, (3, 3))(x)\n",
      "  37:     x = Conv2D(64, (3, 3))(x)\n",
      "  38:     x = Activation('relu')(x)\n",
      "  39:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  40:     x = Dropout(0.25)(x)\n",
      "  41:     x = Flatten()(x)\n",
      "  42:     x = Dense(512)(x)\n",
      "  43:     x = Activation('relu')(x)\n",
      "  44:     x = Dropout(0.5)(x)\n",
      "  45:     x = Dense(n_out)(x)\n",
      "  46:     x = Activation('softmax')(x)\n",
      "  47:     model = Model(inputs=inputs, outputs=x)\n",
      "  48:     model.compile(\n",
      "  49:         loss='categorical_crossentropy',\n",
      "  50:         optimizer=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
      "  51:         metrics=['accuracy'])\n",
      "  52: \n",
      "  53:     model.fit_generator(\n",
      "  54:         g,\n",
      "  55:         steps_per_epoch=steps_per_epoch,\n",
      "  56:         validation_data=(x_test, y_test),\n",
      "  57:         epochs=epochs,\n",
      "  58:         verbose=2,\n",
      "  59:     )\n",
      "  60:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  61:     print('Test accuracy:', acc)\n",
      "  62:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  63: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 4s - loss: 0.6618 - acc: 0.7045 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.5829 - acc: 0.7415 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6326 - acc: 0.6918 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6305 - acc: 0.6960 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.5859 - acc: 0.7443 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6035 - acc: 0.7188 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.6095 - acc: 0.7202 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.6250 - acc: 0.6889 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6135 - acc: 0.7102 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.5974 - acc: 0.7330 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 3\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "\t2: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6327 - acc: 0.7045 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6044 - acc: 0.7216 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.5946 - acc: 0.7287 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6137 - acc: 0.7088 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6004 - acc: 0.7259 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6025 - acc: 0.7145 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.5826 - acc: 0.7330 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.6126 - acc: 0.7116 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6100 - acc: 0.7060 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.5813 - acc: 0.7514 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6553 - acc: 0.6861 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6185 - acc: 0.7102 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6036 - acc: 0.7315 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.5889 - acc: 0.7401 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6304 - acc: 0.6946 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6190 - acc: 0.7159 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.5894 - acc: 0.7401 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.6117 - acc: 0.7145 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6113 - acc: 0.7102 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6021 - acc: 0.7216 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 1\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6499 - acc: 0.7074 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6113 - acc: 0.7202 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6268 - acc: 0.7045 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6140 - acc: 0.7060 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.5887 - acc: 0.7401 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.5784 - acc: 0.7415 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.6210 - acc: 0.7003 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5835 - acc: 0.7315 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.5962 - acc: 0.7173 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6168 - acc: 0.7017 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6237 - acc: 0.7244 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6293 - acc: 0.7017 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6042 - acc: 0.7173 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6089 - acc: 0.7188 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.5965 - acc: 0.7315 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6158 - acc: 0.7060 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.6107 - acc: 0.7188 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5699 - acc: 0.7514 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.5978 - acc: 0.7259 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6004 - acc: 0.7188 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6342 - acc: 0.7159 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6369 - acc: 0.6889 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.5892 - acc: 0.7344 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6279 - acc: 0.7003 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6013 - acc: 0.7188 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6004 - acc: 0.7259 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.5922 - acc: 0.7330 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.6055 - acc: 0.7145 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.5796 - acc: 0.7401 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6000 - acc: 0.7244 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 3\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6612 - acc: 0.6832 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6395 - acc: 0.6875 - val_loss: 8.0590 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      " - 1s - loss: 0.6268 - acc: 0.6960 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6231 - acc: 0.6932 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6169 - acc: 0.7031 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.5962 - acc: 0.7216 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.5997 - acc: 0.7202 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5988 - acc: 0.7273 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6468 - acc: 0.6577 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.5905 - acc: 0.7287 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6888 - acc: 0.6790 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6154 - acc: 0.7116 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6410 - acc: 0.6790 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.5905 - acc: 0.7330 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6013 - acc: 0.7244 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6098 - acc: 0.7074 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.5764 - acc: 0.7457 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.6231 - acc: 0.7003 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6138 - acc: 0.7060 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6020 - acc: 0.7173 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6433 - acc: 0.6989 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.6176 - acc: 0.7060 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6241 - acc: 0.6989 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6134 - acc: 0.7159 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6267 - acc: 0.6918 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.5982 - acc: 0.7230 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.6103 - acc: 0.7173 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5978 - acc: 0.7145 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.5950 - acc: 0.7244 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6184 - acc: 0.7045 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 0.6255 - acc: 0.7031 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 0.5993 - acc: 0.7344 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 0.6329 - acc: 0.6776 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 0.6133 - acc: 0.7102 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.6173 - acc: 0.6960 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.6087 - acc: 0.7159 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.6345 - acc: 0.6776 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5902 - acc: 0.7287 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.6060 - acc: 0.7102 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6330 - acc: 0.6861 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "aug_eyes.search_opt_augment(model=net.neoaug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute data augmentation with best condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "d = os.path.join(aug_brain.dataset.train_dir, 'output')\n",
    "if os.path.exists(d):\n",
    "    shutil.rmtree(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 173_0116.JPG:   0%|          | 2/832 [00:00<01:16, 10.78 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 517 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/train/異常なし/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 108_0189.JPG:   0%|          | 2/832 [00:00<01:08, 12.06 Samples/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 208 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/train/異常あり/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "aug_eyes.augment_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameter of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def data():\n",
      "    train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/auged' \n",
      "    test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/validation'\n",
      "    resize = 96 \n",
      "    rescale = 1 \n",
      "    df = load_data.df_fromdir_classed(train_dir)\n",
      "    x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "    df = load_data.df_fromdir_classed(test_dir)\n",
      "    x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "\n",
      "    y_train = to_categorical(y_train)\n",
      "    y_test = to_categorical(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test\n",
      "\n",
      "            name label             handle  \\\n",
      "0   113_0131.JPG  異常なし  異常なし/113_0131.JPG   \n",
      "1   113_0007.JPG  異常なし  異常なし/113_0007.JPG   \n",
      "2   148_0085.JPG  異常なし  異常なし/148_0085.JPG   \n",
      "3   143_0103.JPG  異常なし  異常なし/143_0103.JPG   \n",
      "4   148_0014.JPG  異常なし  異常なし/148_0014.JPG   \n",
      "5   113_0096.JPG  異常なし  異常なし/113_0096.JPG   \n",
      "6   184_0171.JPG  異常なし  異常なし/184_0171.JPG   \n",
      "7   105_0045.JPG  異常なし  異常なし/105_0045.JPG   \n",
      "8   179_0051.JPG  異常なし  異常なし/179_0051.JPG   \n",
      "9   175_0025.JPG  異常なし  異常なし/175_0025.JPG   \n",
      "10  143_0167.JPG  異常なし  異常なし/143_0167.JPG   \n",
      "11  179_0033.JPG  異常なし  異常なし/179_0033.JPG   \n",
      "12  148_0024.JPG  異常なし  異常なし/148_0024.JPG   \n",
      "13  185_0103.JPG  異常なし  異常なし/185_0103.JPG   \n",
      "14  198_0024.JPG  異常なし  異常なし/198_0024.JPG   \n",
      "15  113_0112.JPG  異常なし  異常なし/113_0112.JPG   \n",
      "16  182_0053.JPG  異常なし  異常なし/182_0053.JPG   \n",
      "17  197_0114.JPG  異常なし  異常なし/197_0114.JPG   \n",
      "18  129_0078.JPG  異常なし  異常なし/129_0078.JPG   \n",
      "19  159_0122.JPG  異常なし  異常なし/159_0122.JPG   \n",
      "20  105_0077.JPG  異常なし  異常なし/105_0077.JPG   \n",
      "21  108_0131.JPG  異常なし  異常なし/108_0131.JPG   \n",
      "22  185_0170.JPG  異常なし  異常なし/185_0170.JPG   \n",
      "23  178_0033.JPG  異常あり  異常あり/178_0033.JPG   \n",
      "24  173_0043.JPG  異常あり  異常あり/173_0043.JPG   \n",
      "25  179_0106.JPG  異常あり  異常あり/179_0106.JPG   \n",
      "26  184_0216.JPG  異常あり  異常あり/184_0216.JPG   \n",
      "27  168_0113.JPG  異常あり  異常あり/168_0113.JPG   \n",
      "28  108_0026.JPG  異常あり  異常あり/108_0026.JPG   \n",
      "29  105_0019.JPG  異常あり  異常あり/105_0019.JPG   \n",
      "30  143_0058.JPG  異常あり  異常あり/143_0058.JPG   \n",
      "31  108_0162.JPG  異常あり  異常あり/108_0162.JPG   \n",
      "32  127_0135.JPG  異常あり  異常あり/127_0135.JPG   \n",
      "33  178_0020.JPG  異常あり  異常あり/178_0020.JPG   \n",
      "34  168_0009.JPG  異常あり  異常あり/168_0009.JPG   \n",
      "35  175_0109.JPG  異常あり  異常あり/175_0109.JPG   \n",
      "36  127_0162.JPG  異常あり  異常あり/127_0162.JPG   \n",
      "37  143_0082.JPG  異常あり  異常あり/143_0082.JPG   \n",
      "38  171_0041.JPG  異常あり  異常あり/171_0041.JPG   \n",
      "39  171_0023.JPG  異常あり  異常あり/171_0023.JPG   \n",
      "40  178_0158.JPG  異常あり  異常あり/178_0158.JPG   \n",
      "41  171_0102.JPG  異常あり  異常あり/171_0102.JPG   \n",
      "42  129_0108.JPG  異常あり  異常あり/129_0108.JPG   \n",
      "43  168_0122.JPG  異常あり  異常あり/168_0122.JPG   \n",
      "44  129_0050.JPG  異常あり  異常あり/129_0050.JPG   \n",
      "45  129_0031.JPG  異常あり  異常あり/129_0031.JPG   \n",
      "\n",
      "                                                 path  \n",
      "0   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "1   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "2   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "3   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "4   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "5   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "6   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "7   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "8   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "9   /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "10  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "11  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "12  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "13  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "14  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "15  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "16  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "17  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "18  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "19  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "20  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "21  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "22  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "23  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "24  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "25  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "26  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "27  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "28  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "29  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "30  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "31  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "32  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "33  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "34  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "35  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "36  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "37  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "38  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "39  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "40  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "41  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "42  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "43  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "44  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      "45  /home/fytroo/Tuner/dev/standard_datasets/5a66d...  \n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import subprocess\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import hp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import BatchNormalization, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Conv2D, Convolution2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, concatenate, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils, to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import Augmentor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import load_data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import net\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas_data import data\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'ch': hp.choice('ch', [16, 32]),\n",
      "        'ch_1': hp.choice('ch_1', [32, 64]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/auged' \n",
      "  3: test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a66d177b037cb1b1503a789/validation'\n",
      "  4: resize = 96 \n",
      "  5: rescale = 1 \n",
      "  6: df = load_data.df_fromdir_classed(train_dir)\n",
      "  7: x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "  8: df = load_data.df_fromdir_classed(test_dir)\n",
      "  9: x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      " 10: \n",
      " 11: y_train = to_categorical(y_train)\n",
      " 12: y_test = to_categorical(y_test)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     params = eval(open('tmp_params.json', 'r').read())\n",
      "   4: \n",
      "   5:     n_out = params['n_out']\n",
      "   6:     input_shape = tuple(params['input_shape'])\n",
      "   7:     batch_size = params['batch_size']\n",
      "   8:     epochs = params['epochs']\n",
      "   9:     lossfun = params['lossfun']\n",
      "  10:     optimizer = params['optimizer']\n",
      "  11:     metrics = ['accuracy']\n",
      "  12: \n",
      "  13:     inputs = Input(shape=input_shape)\n",
      "  14:     x = inputs\n",
      "  15:     ch = space['ch']\n",
      "  16:     x = Conv2D(ch, (3, 3))(x)\n",
      "  17:     x = Conv2D(ch, (3, 3))(x)\n",
      "  18:     x = Activation('relu')(x)\n",
      "  19:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  20:     ch = space['ch_1']\n",
      "  21:     x = Conv2D(ch, (3, 3))(x)\n",
      "  22:     x = Conv2D(ch, (3, 3))(x)\n",
      "  23:     x = Activation('relu')(x)\n",
      "  24:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  25:     x = Dropout(0.25)(x)\n",
      "  26:     x = Flatten()(x)\n",
      "  27:     x = Dense(512)(x)\n",
      "  28:     x = Activation('relu')(x)\n",
      "  29:     x = Dropout(0.5)(x)\n",
      "  30:     x = Dense(n_out)(x)\n",
      "  31:     x = Activation('softmax')(x)\n",
      "  32:     model = Model(inputs=inputs, outputs=x)\n",
      "  33:     model.compile(loss=lossfun, optimizer=optimizer, metrics=metrics)\n",
      "  34:     model.fit(\n",
      "  35:         x_train,\n",
      "  36:         y_train,\n",
      "  37:         batch_size=batch_size,\n",
      "  38:         epochs=epochs,\n",
      "  39:         verbose=2,\n",
      "  40:         validation_data=(x_test, y_test))\n",
      "  41:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  42:     print('Test accuracy:', acc)\n",
      "  43:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  44: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1664 samples, validate on 46 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 8.0596 - acc: 0.4994 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "Train on 1664 samples, validate on 46 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 7.9986 - acc: 0.5024 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "Train on 1664 samples, validate on 46 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 8.0159 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 8.0494 - acc: 0.5006 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "Train on 1664 samples, validate on 46 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 8.0633 - acc: 0.4982 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "Train on 1664 samples, validate on 46 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 8.0176 - acc: 0.5012 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 8.0590 - acc: 0.5000 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n"
     ]
    }
   ],
   "source": [
    "best_condition, best_model = use_hyperas.exec_hyperas(\n",
    "    aug_eyes.augmented_dir,\n",
    "    aug_eyes.dataset.validation_dir,\n",
    "    net.simplenet,\n",
    "    batch_size = 32,\n",
    "    epochs=10,\n",
    "    optimizer='adam'\n",
    ")\n",
    "fname = 'simplenet.eyes.hdf5'\n",
    "best_model.save(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
