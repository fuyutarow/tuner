{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fytroo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import BatchNormalization, Flatten, Dense\n",
    "from keras.layers import Input, Conv2D, Convolution2D\n",
    "from keras.layers import Activation, concatenate, Dropout\n",
    "from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "import Augmentor\n",
    "import json\n",
    "\n",
    "import tuner\n",
    "from tuner import utils\n",
    "from tuner import load_data\n",
    "from tuner import augment_data\n",
    "from tuner import use_hyperas\n",
    "from tuner import net\n",
    "from tuner.dataset import ClassificationDataset, AugmentDataset\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "params = EasyDict({\n",
    "    'src_dir': '../micin-dataset/brain',\n",
    "    'dst_dir': './brain',\n",
    "    'bs': 32,\n",
    "    'epochs': 10,\n",
    "    'save_model_file': 'tuner.{}.model.hdf5'.format(int(datetime.now().timestamp()))\n",
    "})\n",
    "try:\n",
    "    __file__\n",
    "    import argparse\n",
    "    argparse = argparse.ArgumentParser()\n",
    "    parser.add_argument('-d','--dataset', dest='src_dir', type=str,\n",
    "                         help='dataset directory')\n",
    "    parser.add_argument('-o','--output', dest='save_model_file', type=str, default=params.save_model_file,\n",
    "                         help='dataset directory')\n",
    "    parser.add_argument('-b','--batchsize', dest='bs', type=int, default=params.bs,\n",
    "                         help='batch size')\n",
    "    parser.add_argument('-e','--epochs', type=int, default=params.epochs,\n",
    "                         help='epochs')\n",
    "    args = parser.parse_args()\n",
    "except:\n",
    "    args = params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert  dataset from brain-dir to ready-dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = load_data.df_fromdir_brain(args.src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_data.classed_dir_fromdf(df, \"tmp/brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset via ready-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain = ClassificationDataset('tmp/brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain = AugmentDataset(brain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/auged'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.augmented_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/train'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brain.dataset.train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search best condition of data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def data():\n",
      "    train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/train'\n",
      "    test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/validation'\n",
      "    resize = '96'\n",
      "    rescale = '1'\n",
      "    df = load_data.df_fromdir_classed(train_dir)\n",
      "    x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "    df = load_data.df_fromdir_classed(test_dir)\n",
      "    x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "\n",
      "    y_train = to_categorical(y_train)\n",
      "    y_test = to_categorical(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test\n",
      "\n",
      "         name label        handle  \\\n",
      "0    MS83.jpg    MS   MS/MS83.jpg   \n",
      "1   MS068.jpg    MS  MS/MS068.jpg   \n",
      "2   MS135.jpg    MS  MS/MS135.jpg   \n",
      "3   MS167.jpg    MS  MS/MS167.jpg   \n",
      "4   MS123.jpg    MS  MS/MS123.jpg   \n",
      "5   PD050.jpg    PD  PD/PD050.jpg   \n",
      "6   PD079.jpg    PD  PD/PD079.jpg   \n",
      "7   PD102.jpg    PD  PD/PD102.jpg   \n",
      "8   PD062.jpg    PD  PD/PD062.jpg   \n",
      "9   PD077.jpg    PD  PD/PD077.jpg   \n",
      "10   N028.jpg     N    N/N028.jpg   \n",
      "11   N127.jpg     N    N/N127.jpg   \n",
      "12   N118.jpg     N    N/N118.jpg   \n",
      "13   N144.jpg     N    N/N144.jpg   \n",
      "14   N019.jpg     N    N/N019.jpg   \n",
      "15  PS060.jpg    PS  PS/PS060.jpg   \n",
      "16  PS100.jpg    PS  PS/PS100.jpg   \n",
      "17  PS110.jpg    PS  PS/PS110.jpg   \n",
      "18  PS115.jpg    PS  PS/PS115.jpg   \n",
      "19  PS113.jpg    PS  PS/PS113.jpg   \n",
      "\n",
      "                                                 path  \n",
      "0   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "1   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "2   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "3   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "4   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "5   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "6   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "7   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "8   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "9   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "10  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "11  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "12  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "13  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "14  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "15  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "16  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "17  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "18  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "19  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import subprocess\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import hp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import BatchNormalization, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Conv2D, Convolution2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, concatenate, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils, to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import Augmentor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import load_data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import net\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas_data import data\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'conditional': hp.choice('conditional', [True, False]),\n",
      "        'conditional_1': hp.choice('conditional_1', [True, False]),\n",
      "        'conditional_2': hp.choice('conditional_2', [True, False]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/train'\n",
      "  3: test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/validation'\n",
      "  4: resize = '96'\n",
      "  5: rescale = '1'\n",
      "  6: df = load_data.df_fromdir_classed(train_dir)\n",
      "  7: x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "  8: df = load_data.df_fromdir_classed(test_dir)\n",
      "  9: x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      " 10: \n",
      " 11: y_train = to_categorical(y_train)\n",
      " 12: y_test = to_categorical(y_test)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     params = eval(open('tmp_params.json', 'r').read())\n",
      "   4: \n",
      "   5:     n_out = params['n_out']\n",
      "   6:     input_shape = tuple(params['input_shape'])\n",
      "   7:     batch_size = params['batch_size']\n",
      "   8:     epochs = params['epochs']\n",
      "   9:     lossfun = params['lossfun']\n",
      "  10:     optimizer = params['optimizer']\n",
      "  11:     metrics = ['accuracy']\n",
      "  12: \n",
      "  13:     steps_per_epoch = len(x_train) // batch_size\n",
      "  14: \n",
      "  15:     p = Augmentor.Pipeline()\n",
      "  16: \n",
      "  17:     p.flip_left_right(probability=0.5)\n",
      "  18:     if conditional(space['conditional']):\n",
      "  19:         p.crop_random(probability=1, percentage_area=0.8)\n",
      "  20:         p.resize(probability=1, width=96, height=96)\n",
      "  21:     if conditional(space['conditional_1']):\n",
      "  22:         p.random_erasing(probability=0.5, rectangle_area=0.2)\n",
      "  23:     if conditional(space['conditional_2']):\n",
      "  24:         p.shear(probability=0.3, max_shear_left=2, max_shear_right=2)\n",
      "  25:     print('-' * 80)\n",
      "  26:     p.status()\n",
      "  27: \n",
      "  28:     g = p.keras_generator_from_array(x_train.astype(np.uint8), y_train, batch_size=batch_size)\n",
      "  29:     g = ((x / 255., y) for (x, y) in g)\n",
      "  30: \n",
      "  31:     inputs = Input(shape=input_shape)\n",
      "  32:     x = inputs\n",
      "  33:     x = Conv2D(32, (3, 3))(x)\n",
      "  34:     x = Conv2D(32, (3, 3))(x)\n",
      "  35:     x = Activation('relu')(x)\n",
      "  36:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  37:     x = Conv2D(64, (3, 3))(x)\n",
      "  38:     x = Conv2D(64, (3, 3))(x)\n",
      "  39:     x = Activation('relu')(x)\n",
      "  40:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  41:     x = Dropout(0.25)(x)\n",
      "  42:     x = Flatten()(x)\n",
      "  43:     x = Dense(512)(x)\n",
      "  44:     x = Activation('relu')(x)\n",
      "  45:     x = Dropout(0.5)(x)\n",
      "  46:     x = Dense(n_out)(x)\n",
      "  47:     x = Activation('softmax')(x)\n",
      "  48:     model = Model(inputs=inputs, outputs=x)\n",
      "  49:     model.compile(\n",
      "  50:         loss='categorical_crossentropy',\n",
      "  51:         optimizer=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
      "  52:         metrics=['accuracy'])\n",
      "  53: \n",
      "  54:     model.fit_generator(\n",
      "  55:         g,\n",
      "  56:         steps_per_epoch=steps_per_epoch,\n",
      "  57:         validation_data=(x_test, y_test),\n",
      "  58:         epochs=epochs,\n",
      "  59:         verbose=2,\n",
      "  60:     )\n",
      "  61:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  62:     print('Test accuracy:', acc)\n",
      "  63:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  64: \n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 3s - loss: 1.5466 - acc: 0.2946 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3957 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3574 - acc: 0.2723 - val_loss: 11.4374 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3661 - acc: 0.3214 - val_loss: 10.6993 - val_acc: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      " - 0s - loss: 1.3839 - acc: 0.2545 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3344 - acc: 0.3170 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3676 - acc: 0.3705 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3322 - acc: 0.3884 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.3666 - acc: 0.3080 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3565 - acc: 0.3259 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 3\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "\t2: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.3882 - acc: 0.3125 - val_loss: 11.2616 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3213 - acc: 0.3839 - val_loss: 10.8343 - val_acc: 0.3000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2649 - acc: 0.4420 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2838 - acc: 0.4286 - val_loss: 11.2827 - val_acc: 0.3000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2483 - acc: 0.4509 - val_loss: 8.7500 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1400 - acc: 0.6071 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.1170 - acc: 0.5446 - val_loss: 8.8666 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.0333 - acc: 0.6429 - val_loss: 8.0721 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.9884 - acc: 0.6429 - val_loss: 7.2535 - val_acc: 0.5500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.0189 - acc: 0.5804 - val_loss: 7.9262 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.4265 - acc: 0.2634 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3590 - acc: 0.3125 - val_loss: 11.7449 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3603 - acc: 0.3616 - val_loss: 10.8018 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3710 - acc: 0.2857 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.3360 - acc: 0.3527 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3583 - acc: 0.3214 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3559 - acc: 0.3571 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3553 - acc: 0.3705 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.3426 - acc: 0.3750 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3720 - acc: 0.3125 - val_loss: 11.6416 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 1\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.4781 - acc: 0.2902 - val_loss: 11.7466 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3328 - acc: 0.3884 - val_loss: 10.9784 - val_acc: 0.3000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3508 - acc: 0.3661 - val_loss: 10.4800 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2434 - acc: 0.4866 - val_loss: 10.4771 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.1313 - acc: 0.5938 - val_loss: 8.8653 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1548 - acc: 0.4643 - val_loss: 8.0600 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.0674 - acc: 0.6027 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.9689 - acc: 0.6384 - val_loss: 8.0984 - val_acc: 0.4500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.9255 - acc: 0.6473 - val_loss: 8.0137 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.8057 - acc: 0.7143 - val_loss: 8.0806 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.4626 - acc: 0.2679 - val_loss: 10.4937 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3509 - acc: 0.3661 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2511 - acc: 0.4464 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2106 - acc: 0.5000 - val_loss: 10.4332 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2143 - acc: 0.5402 - val_loss: 7.8356 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1051 - acc: 0.5402 - val_loss: 8.0592 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.0458 - acc: 0.5848 - val_loss: 8.8665 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.9706 - acc: 0.6116 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.8386 - acc: 0.7098 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.8928 - acc: 0.6518 - val_loss: 6.5327 - val_acc: 0.5500\n",
      "Test accuracy: 0.550000011921\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.4100 - acc: 0.3080 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3950 - acc: 0.2857 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3719 - acc: 0.3705 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3627 - acc: 0.3304 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.3743 - acc: 0.3125 - val_loss: 11.3134 - val_acc: 0.3000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3497 - acc: 0.3527 - val_loss: 9.0691 - val_acc: 0.3500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3801 - acc: 0.2723 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3756 - acc: 0.3125 - val_loss: 8.9387 - val_acc: 0.3500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.3384 - acc: 0.3616 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3057 - acc: 0.3571 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 3\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.4376 - acc: 0.2812 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3638 - acc: 0.3482 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.4027 - acc: 0.2411 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3402 - acc: 0.3170 - val_loss: 11.8428 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.3372 - acc: 0.3705 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3520 - acc: 0.3259 - val_loss: 10.0818 - val_acc: 0.2000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3526 - acc: 0.3571 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3271 - acc: 0.3929 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.3606 - acc: 0.3884 - val_loss: 11.0116 - val_acc: 0.2000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3431 - acc: 0.3661 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 1.4264 - acc: 0.3705 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3586 - acc: 0.3839 - val_loss: 11.2827 - val_acc: 0.3000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3172 - acc: 0.4107 - val_loss: 12.0860 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2878 - acc: 0.4509 - val_loss: 11.9174 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2621 - acc: 0.4241 - val_loss: 10.7539 - val_acc: 0.3000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.2238 - acc: 0.4643 - val_loss: 11.8330 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.1505 - acc: 0.5580 - val_loss: 11.2827 - val_acc: 0.3000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.1225 - acc: 0.5536 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.0552 - acc: 0.6116 - val_loss: 9.5703 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.9799 - acc: 0.6250 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Test accuracy: 0.40000000596\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.3653 - acc: 0.3571 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3406 - acc: 0.3884 - val_loss: 10.4769 - val_acc: 0.3500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2658 - acc: 0.4911 - val_loss: 11.9901 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2651 - acc: 0.4286 - val_loss: 10.7787 - val_acc: 0.3000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.0948 - acc: 0.6250 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1659 - acc: 0.5179 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.0285 - acc: 0.6161 - val_loss: 6.8225 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.0149 - acc: 0.6384 - val_loss: 7.2531 - val_acc: 0.5500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.9183 - acc: 0.6339 - val_loss: 5.6413 - val_acc: 0.6500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.8592 - acc: 0.6875 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Test accuracy: 0.600000023842\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.4823 - acc: 0.2946 - val_loss: 11.9884 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3677 - acc: 0.3393 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3064 - acc: 0.4241 - val_loss: 11.2827 - val_acc: 0.3000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2931 - acc: 0.4821 - val_loss: 10.9540 - val_acc: 0.3000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2260 - acc: 0.5446 - val_loss: 10.4925 - val_acc: 0.3500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1253 - acc: 0.5759 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.0960 - acc: 0.5580 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.0374 - acc: 0.6205 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.9894 - acc: 0.6250 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.8666 - acc: 0.6741 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Test accuracy: 0.40000000596\n"
     ]
    }
   ],
   "source": [
    "brain.search_opt_augment(model=net.neoaug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute data augmentation with best condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MS070.jpg:   5%|▌         | 10/184 [00:00<00:02, 59.00 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 46 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/train/MS/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PD044.jpg:   3%|▎         | 5/184 [00:00<00:05, 33.74 Samples/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 46 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/train/PD/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing N080.jpg:  10%|█         | 19/184 [00:00<00:01, 102.79 Samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 79 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/train/N/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PS107.jpg:   3%|▎         | 5/184 [00:00<00:05, 33.85 Samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 75 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/train/PS/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    }
   ],
   "source": [
    "brain.augment_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameter of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def data():\n",
      "    train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/auged'\n",
      "    test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/validation'\n",
      "    resize = '96'\n",
      "    rescale = '0.00392156862745098'\n",
      "    df = load_data.df_fromdir_classed(train_dir)\n",
      "    x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "    df = load_data.df_fromdir_classed(test_dir)\n",
      "    x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "\n",
      "    y_train = to_categorical(y_train)\n",
      "    y_test = to_categorical(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test\n",
      "\n",
      "         name label        handle  \\\n",
      "0    MS83.jpg    MS   MS/MS83.jpg   \n",
      "1   MS068.jpg    MS  MS/MS068.jpg   \n",
      "2   MS135.jpg    MS  MS/MS135.jpg   \n",
      "3   MS167.jpg    MS  MS/MS167.jpg   \n",
      "4   MS123.jpg    MS  MS/MS123.jpg   \n",
      "5   PD050.jpg    PD  PD/PD050.jpg   \n",
      "6   PD079.jpg    PD  PD/PD079.jpg   \n",
      "7   PD102.jpg    PD  PD/PD102.jpg   \n",
      "8   PD062.jpg    PD  PD/PD062.jpg   \n",
      "9   PD077.jpg    PD  PD/PD077.jpg   \n",
      "10   N028.jpg     N    N/N028.jpg   \n",
      "11   N127.jpg     N    N/N127.jpg   \n",
      "12   N118.jpg     N    N/N118.jpg   \n",
      "13   N144.jpg     N    N/N144.jpg   \n",
      "14   N019.jpg     N    N/N019.jpg   \n",
      "15  PS060.jpg    PS  PS/PS060.jpg   \n",
      "16  PS100.jpg    PS  PS/PS100.jpg   \n",
      "17  PS110.jpg    PS  PS/PS110.jpg   \n",
      "18  PS115.jpg    PS  PS/PS115.jpg   \n",
      "19  PS113.jpg    PS  PS/PS113.jpg   \n",
      "\n",
      "                                                 path  \n",
      "0   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "1   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "2   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "3   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "4   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "5   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "6   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "7   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "8   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "9   /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "10  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "11  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "12  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "13  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "14  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "15  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "16  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "17  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "18  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      "19  /home/fytroo/Tuner/dev/standard_datasets/5a670...  \n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import subprocess\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import hp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import BatchNormalization, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Conv2D, Convolution2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, concatenate, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils, to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import Augmentor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import load_data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import net\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas_data import data\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'ch': hp.choice('ch', [16, 32]),\n",
      "        'ch_1': hp.choice('ch_1', [32, 64]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: train_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/auged'\n",
      "  3: test_dir = '/home/fytroo/Tuner/dev/standard_datasets/5a67031cb037cb528757c444/validation'\n",
      "  4: resize = '96'\n",
      "  5: rescale = '0.00392156862745098'\n",
      "  6: df = load_data.df_fromdir_classed(train_dir)\n",
      "  7: x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "  8: df = load_data.df_fromdir_classed(test_dir)\n",
      "  9: x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      " 10: \n",
      " 11: y_train = to_categorical(y_train)\n",
      " 12: y_test = to_categorical(y_test)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     params = eval(open('tmp_params.json', 'r').read())\n",
      "   4: \n",
      "   5:     n_out = params['n_out']\n",
      "   6:     input_shape = tuple(params['input_shape'])\n",
      "   7:     batch_size = params['batch_size']\n",
      "   8:     epochs = params['epochs']\n",
      "   9:     lossfun = params['lossfun']\n",
      "  10:     optimizer = params['optimizer']\n",
      "  11:     metrics = ['accuracy']\n",
      "  12: \n",
      "  13:     inputs = Input(shape=input_shape)\n",
      "  14:     x = inputs\n",
      "  15:     ch = space['ch']\n",
      "  16:     x = Conv2D(ch, (3, 3))(x)\n",
      "  17:     x = Conv2D(ch, (3, 3))(x)\n",
      "  18:     x = Activation('relu')(x)\n",
      "  19:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  20:     ch = space['ch_1']\n",
      "  21:     x = Conv2D(ch, (3, 3))(x)\n",
      "  22:     x = Conv2D(ch, (3, 3))(x)\n",
      "  23:     x = Activation('relu')(x)\n",
      "  24:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  25:     x = Dropout(0.25)(x)\n",
      "  26:     x = Flatten()(x)\n",
      "  27:     x = Dense(512)(x)\n",
      "  28:     x = Activation('relu')(x)\n",
      "  29:     x = Dropout(0.5)(x)\n",
      "  30:     x = Dense(n_out)(x)\n",
      "  31:     x = Activation('softmax')(x)\n",
      "  32:     model = Model(inputs=inputs, outputs=x)\n",
      "  33:     model.compile(loss=lossfun, optimizer=optimizer, metrics=metrics)\n",
      "  34:     model.fit(\n",
      "  35:         x_train,\n",
      "  36:         y_train,\n",
      "  37:         batch_size=batch_size,\n",
      "  38:         epochs=epochs,\n",
      "  39:         verbose=2,\n",
      "  40:         validation_data=(x_test, y_test))\n",
      "  41:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  42:     print('Test accuracy:', acc)\n",
      "  43:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  44: \n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.4039 - acc: 0.2323 - val_loss: 1.3841 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3872 - acc: 0.2649 - val_loss: 1.3847 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.3838 - acc: 0.2636 - val_loss: 1.3787 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.3769 - acc: 0.2962 - val_loss: 1.3848 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.3389 - acc: 0.3546 - val_loss: 1.3979 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 1.2599 - acc: 0.4348 - val_loss: 1.5165 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 1.1502 - acc: 0.5082 - val_loss: 1.6716 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 1.0139 - acc: 0.5679 - val_loss: 1.4121 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.8341 - acc: 0.6603 - val_loss: 1.8641 - val_acc: 0.3500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6806 - acc: 0.7242 - val_loss: 1.6882 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.5504 - acc: 0.2242 - val_loss: 1.3802 - val_acc: 0.3000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3535 - acc: 0.3533 - val_loss: 1.3308 - val_acc: 0.4000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2429 - acc: 0.4375 - val_loss: 1.3142 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.1426 - acc: 0.5122 - val_loss: 1.9232 - val_acc: 0.3000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.9463 - acc: 0.6101 - val_loss: 1.7296 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.7120 - acc: 0.7296 - val_loss: 1.8777 - val_acc: 0.4500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.5615 - acc: 0.7921 - val_loss: 2.0796 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.3734 - acc: 0.8655 - val_loss: 2.1492 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.2713 - acc: 0.8981 - val_loss: 3.1651 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.2042 - acc: 0.9266 - val_loss: 3.5443 - val_acc: 0.4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.40000000596\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.4356 - acc: 0.2649 - val_loss: 1.3765 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3690 - acc: 0.3071 - val_loss: 1.3037 - val_acc: 0.4000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.2907 - acc: 0.3967 - val_loss: 1.5654 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.1773 - acc: 0.4932 - val_loss: 1.3749 - val_acc: 0.4000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.9796 - acc: 0.5856 - val_loss: 2.2032 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.8010 - acc: 0.6848 - val_loss: 2.0357 - val_acc: 0.4000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.5693 - acc: 0.8071 - val_loss: 3.1917 - val_acc: 0.3500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.3462 - acc: 0.8723 - val_loss: 2.7005 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.2152 - acc: 0.9321 - val_loss: 4.1056 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.1615 - acc: 0.9511 - val_loss: 4.4698 - val_acc: 0.3500\n",
      "Test accuracy: 0.34999999404\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.4687 - acc: 0.2242 - val_loss: 1.3860 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3870 - acc: 0.2269 - val_loss: 1.3857 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.3862 - acc: 0.2582 - val_loss: 1.3827 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.3746 - acc: 0.2921 - val_loss: 1.4274 - val_acc: 0.3000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.3138 - acc: 0.3940 - val_loss: 1.3795 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 1.2574 - acc: 0.4090 - val_loss: 1.4508 - val_acc: 0.3500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 1.1784 - acc: 0.4783 - val_loss: 1.4256 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 1.0393 - acc: 0.5543 - val_loss: 1.4174 - val_acc: 0.4500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.8603 - acc: 0.6196 - val_loss: 1.7481 - val_acc: 0.3500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6138 - acc: 0.7704 - val_loss: 1.8101 - val_acc: 0.4500\n",
      "Test accuracy: 0.449999988079\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.5248 - acc: 0.2568 - val_loss: 1.3840 - val_acc: 0.3000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3812 - acc: 0.2962 - val_loss: 1.3748 - val_acc: 0.3000\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.3512 - acc: 0.3533 - val_loss: 1.4081 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.3301 - acc: 0.3533 - val_loss: 1.4511 - val_acc: 0.3000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.2341 - acc: 0.4429 - val_loss: 1.5257 - val_acc: 0.3000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 1.2146 - acc: 0.4511 - val_loss: 1.3231 - val_acc: 0.4500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 1.0937 - acc: 0.5489 - val_loss: 1.6397 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.9653 - acc: 0.6141 - val_loss: 1.4933 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.8402 - acc: 0.6630 - val_loss: 2.1493 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6648 - acc: 0.7446 - val_loss: 1.7711 - val_acc: 0.4500\n",
      "Test accuracy: 0.449999988079\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.4097 - acc: 0.2704 - val_loss: 1.3856 - val_acc: 0.3500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3838 - acc: 0.2690 - val_loss: 1.3793 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.3481 - acc: 0.3723 - val_loss: 1.2605 - val_acc: 0.4500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.2249 - acc: 0.4389 - val_loss: 1.2738 - val_acc: 0.4000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.1108 - acc: 0.5367 - val_loss: 1.3633 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.9343 - acc: 0.6372 - val_loss: 1.2431 - val_acc: 0.6000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.6913 - acc: 0.7446 - val_loss: 1.7031 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5071 - acc: 0.8179 - val_loss: 1.6115 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.3612 - acc: 0.8872 - val_loss: 1.5987 - val_acc: 0.5500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.2504 - acc: 0.9212 - val_loss: 2.2044 - val_acc: 0.6500\n",
      "Test accuracy: 0.649999976158\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.4685 - acc: 0.2391 - val_loss: 1.3865 - val_acc: 0.2000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3866 - acc: 0.2541 - val_loss: 1.3861 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.3861 - acc: 0.2568 - val_loss: 1.3854 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.3813 - acc: 0.3043 - val_loss: 1.3711 - val_acc: 0.4500\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.3447 - acc: 0.3601 - val_loss: 1.2660 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 1.2838 - acc: 0.4185 - val_loss: 1.3120 - val_acc: 0.3500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 1.1647 - acc: 0.5054 - val_loss: 1.4004 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 1.0260 - acc: 0.5652 - val_loss: 1.3006 - val_acc: 0.4500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.8560 - acc: 0.6766 - val_loss: 1.2459 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6023 - acc: 0.7704 - val_loss: 1.5550 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.5475 - acc: 0.2677 - val_loss: 1.3968 - val_acc: 0.3000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3859 - acc: 0.2785 - val_loss: 1.3912 - val_acc: 0.3500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.3790 - acc: 0.2758 - val_loss: 1.3806 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.3676 - acc: 0.3533 - val_loss: 1.3460 - val_acc: 0.4000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.3020 - acc: 0.4035 - val_loss: 1.2902 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 1.2248 - acc: 0.4484 - val_loss: 1.1984 - val_acc: 0.4500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 1.0860 - acc: 0.5326 - val_loss: 1.2373 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.9705 - acc: 0.5978 - val_loss: 1.6377 - val_acc: 0.3000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.8646 - acc: 0.6427 - val_loss: 1.8228 - val_acc: 0.3500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.6999 - acc: 0.7188 - val_loss: 1.9907 - val_acc: 0.4000\n",
      "Test accuracy: 0.40000000596\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.3970 - acc: 0.2351 - val_loss: 1.3805 - val_acc: 0.3000\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3689 - acc: 0.3193 - val_loss: 1.3103 - val_acc: 0.4500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.3081 - acc: 0.3750 - val_loss: 1.2738 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.1763 - acc: 0.4538 - val_loss: 1.2138 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.0516 - acc: 0.5476 - val_loss: 1.4678 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.9077 - acc: 0.6250 - val_loss: 1.1974 - val_acc: 0.4500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.7274 - acc: 0.7106 - val_loss: 1.2777 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.5166 - acc: 0.8016 - val_loss: 0.9547 - val_acc: 0.7000\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.3195 - acc: 0.8832 - val_loss: 1.2107 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.1586 - acc: 0.9484 - val_loss: 1.9350 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 1.4167 - acc: 0.2622 - val_loss: 1.3878 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.3510 - acc: 0.3410 - val_loss: 1.3612 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 1.2487 - acc: 0.4158 - val_loss: 1.2736 - val_acc: 0.4000\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.1177 - acc: 0.5272 - val_loss: 1.1865 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 1s - loss: 0.9456 - acc: 0.6318 - val_loss: 1.3776 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      " - 1s - loss: 0.7403 - acc: 0.7296 - val_loss: 1.5571 - val_acc: 0.4000\n",
      "Epoch 7/10\n",
      " - 1s - loss: 0.4794 - acc: 0.8220 - val_loss: 1.9962 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 1s - loss: 0.3436 - acc: 0.8818 - val_loss: 2.5250 - val_acc: 0.4500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 0.2014 - acc: 0.9375 - val_loss: 2.2043 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 1s - loss: 0.1610 - acc: 0.9497 - val_loss: 3.4168 - val_acc: 0.4500\n",
      "Test accuracy: 0.449999988079\n"
     ]
    }
   ],
   "source": [
    "best_condition, best_model = use_hyperas.exec_hyperas(\n",
    "    brain.augmented_dir,\n",
    "    brain.dataset.validation_dir,\n",
    "    net.simplenet,\n",
    "    batch_size = 32,\n",
    "    epochs=10,\n",
    "    optimizer='adam',\n",
    "    rescale=1./255\n",
    ")\n",
    "best_model.save(args.save_model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
