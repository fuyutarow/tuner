{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/fytroo/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform, conditional\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import BatchNormalization, Flatten, Dense\n",
    "from keras.layers import Input, Conv2D, Convolution2D\n",
    "from keras.layers import Activation, concatenate, Dropout\n",
    "from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "\n",
    "import Augmentor\n",
    "import json\n",
    "\n",
    "import tuner\n",
    "from tuner import utils\n",
    "from tuner import load_data\n",
    "from tuner import augment_data\n",
    "from tuner import use_hyperas\n",
    "from tuner import net\n",
    "from tuner.dataset import ClassificationDataset, AugmentDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from easydict import EasyDict\n",
    "params = EasyDict({\n",
    "    'src_dir': '../micin-dataset/brain',\n",
    "    'dst_dir': './brain'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert  dataset from brain-dir to ready-dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = load_data.df_fromdir_brain(params.src_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "load_data.classed_dir_fromdf(df, \"tmp/brain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset via ready-dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain = ClassificationDataset('tmp/brain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "brain_dataset = brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(brain_dataset.counts_train_data().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 246 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train/output."
     ]
    }
   ],
   "source": [
    "aug_brain = AugmentDataset(brain_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/auged'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_brain.augmented_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aug_brain.dataset.train_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search best condition of data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def data():\n",
      "    train_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train' \n",
      "    test_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/validation'\n",
      "    resize = 96 \n",
      "    rescale = 1 \n",
      "    df = load_data.df_fromdir_classed(train_dir)\n",
      "    x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "    df = load_data.df_fromdir_classed(test_dir)\n",
      "    x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "\n",
      "    y_train = to_categorical(y_train)\n",
      "    y_test = to_categorical(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test\n",
      "\n",
      "         name label        handle  \\\n",
      "0   MS140.jpg    MS  MS/MS140.jpg   \n",
      "1   MS162.jpg    MS  MS/MS162.jpg   \n",
      "2   MS068.jpg    MS  MS/MS068.jpg   \n",
      "3   MS124.jpg    MS  MS/MS124.jpg   \n",
      "4   MS059.jpg    MS  MS/MS059.jpg   \n",
      "5   PD043.jpg    PD  PD/PD043.jpg   \n",
      "6   PD114.jpg    PD  PD/PD114.jpg   \n",
      "7   PD092.jpg    PD  PD/PD092.jpg   \n",
      "8   PD002.jpg    PD  PD/PD002.jpg   \n",
      "9   PD096.jpg    PD  PD/PD096.jpg   \n",
      "10   N160.jpg     N    N/N160.jpg   \n",
      "11   N120.jpg     N    N/N120.jpg   \n",
      "12   N025.jpg     N    N/N025.jpg   \n",
      "13   N035.jpg     N    N/N035.jpg   \n",
      "14   N061.jpg     N    N/N061.jpg   \n",
      "15  PS112.jpg    PS  PS/PS112.jpg   \n",
      "16  PS042.jpg    PS  PS/PS042.jpg   \n",
      "17   PS72.jpg    PS   PS/PS72.jpg   \n",
      "18  PS126.jpg    PS  PS/PS126.jpg   \n",
      "19   PS87.jpg    PS   PS/PS87.jpg   \n",
      "\n",
      "                                                 path  \n",
      "0   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "1   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "2   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "3   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "4   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "5   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "6   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "7   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "8   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "9   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "10  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "11  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "12  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "13  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "14  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "15  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "16  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "17  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "18  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "19  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import subprocess\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import hp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import BatchNormalization, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Conv2D, Convolution2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, concatenate, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils, to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import Augmentor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import load_data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import net\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas_data import data\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'conditional': hp.choice('conditional', [True, False]),\n",
      "        'conditional_1': hp.choice('conditional_1', [True, False]),\n",
      "        'conditional_2': hp.choice('conditional_2', [True, False]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: train_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train' \n",
      "  3: test_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/validation'\n",
      "  4: resize = 96 \n",
      "  5: rescale = 1 \n",
      "  6: df = load_data.df_fromdir_classed(train_dir)\n",
      "  7: x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "  8: df = load_data.df_fromdir_classed(test_dir)\n",
      "  9: x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      " 10: \n",
      " 11: y_train = to_categorical(y_train)\n",
      " 12: y_test = to_categorical(y_test)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     params = eval(open('tmp_params.json', 'r').read())\n",
      "   4: \n",
      "   5:     n_out = params['n_out']\n",
      "   6:     input_shape = tuple(params['input_shape'])\n",
      "   7:     batch_size = params['batch_size']\n",
      "   8:     epochs = params['epochs']\n",
      "   9:     lossfun = params['lossfun']\n",
      "  10:     optimizer = params['optimizer']\n",
      "  11:     metrics = ['accuracy']\n",
      "  12: \n",
      "  13:     steps_per_epoch = len(x_train) // batch_size\n",
      "  14: \n",
      "  15:     p = Augmentor.Pipeline()\n",
      "  16: \n",
      "  17:     p.flip_left_right(probability=0.5)\n",
      "  18:     if conditional(space['conditional']):\n",
      "  19:         p.crop_random(probability=1, percentage_area=0.8)\n",
      "  20:         p.resize(probability=1, width=96, height=96)\n",
      "  21:     if conditional(space['conditional_1']):\n",
      "  22:         p.random_erasing(probability=0.5, rectangle_area=0.2)\n",
      "  23:     if conditional(space['conditional_2']):\n",
      "  24:         p.shear(probability=0.3, max_shear_left=2, max_shear_right=2)\n",
      "  25:     print('-' * 80)\n",
      "  26:     p.status()\n",
      "  27:     g = p.keras_generator_from_array(x_train, y_train, batch_size=batch_size)\n",
      "  28:     g = ((x / 255., y) for (x, y) in g)\n",
      "  29: \n",
      "  30:     inputs = Input(shape=input_shape)\n",
      "  31:     x = inputs\n",
      "  32:     x = Conv2D(32, (3, 3))(x)\n",
      "  33:     x = Conv2D(32, (3, 3))(x)\n",
      "  34:     x = Activation('relu')(x)\n",
      "  35:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  36:     x = Conv2D(64, (3, 3))(x)\n",
      "  37:     x = Conv2D(64, (3, 3))(x)\n",
      "  38:     x = Activation('relu')(x)\n",
      "  39:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  40:     x = Dropout(0.25)(x)\n",
      "  41:     x = Flatten()(x)\n",
      "  42:     x = Dense(512)(x)\n",
      "  43:     x = Activation('relu')(x)\n",
      "  44:     x = Dropout(0.5)(x)\n",
      "  45:     x = Dense(n_out)(x)\n",
      "  46:     x = Activation('softmax')(x)\n",
      "  47:     model = Model(inputs=inputs, outputs=x)\n",
      "  48:     model.compile(\n",
      "  49:         loss='categorical_crossentropy',\n",
      "  50:         optimizer=keras.optimizers.rmsprop(lr=0.0001, decay=1e-6),\n",
      "  51:         metrics=['accuracy'])\n",
      "  52: \n",
      "  53:     model.fit_generator(\n",
      "  54:         g,\n",
      "  55:         steps_per_epoch=steps_per_epoch,\n",
      "  56:         validation_data=(x_test, y_test),\n",
      "  57:         epochs=epochs,\n",
      "  58:         verbose=2,\n",
      "  59:     )\n",
      "  60:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  61:     print('Test accuracy:', acc)\n",
      "  62:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  63: \n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 3s - loss: 1.3993 - acc: 0.2946 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3524 - acc: 0.3839 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3644 - acc: 0.3036 - val_loss: 9.5019 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3527 - acc: 0.3705 - val_loss: 10.3323 - val_acc: 0.3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10\n",
      " - 0s - loss: 1.3307 - acc: 0.3616 - val_loss: 8.5675 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3546 - acc: 0.3661 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3479 - acc: 0.3661 - val_loss: 9.5594 - val_acc: 0.3500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3487 - acc: 0.3616 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.3385 - acc: 0.3973 - val_loss: 9.0588 - val_acc: 0.4000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3454 - acc: 0.3750 - val_loss: 9.9302 - val_acc: 0.3500\n",
      "Test accuracy: 0.34999999404\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 3\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "\t2: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.4434 - acc: 0.3393 - val_loss: 9.8870 - val_acc: 0.3500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.2613 - acc: 0.4509 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3247 - acc: 0.3929 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2680 - acc: 0.4152 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2446 - acc: 0.4554 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1882 - acc: 0.4955 - val_loss: 8.4154 - val_acc: 0.4500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.1069 - acc: 0.5848 - val_loss: 8.1599 - val_acc: 0.4500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.0685 - acc: 0.5982 - val_loss: 6.4584 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.0150 - acc: 0.6205 - val_loss: 9.9186 - val_acc: 0.3500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.0003 - acc: 0.5402 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.4516 - acc: 0.2946 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3831 - acc: 0.2991 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3744 - acc: 0.3214 - val_loss: 11.7496 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3668 - acc: 0.2902 - val_loss: 9.0580 - val_acc: 0.4000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.3742 - acc: 0.2857 - val_loss: 11.5424 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3610 - acc: 0.3348 - val_loss: 11.7864 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3639 - acc: 0.3214 - val_loss: 7.4194 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3207 - acc: 0.3929 - val_loss: 9.5671 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.2958 - acc: 0.4286 - val_loss: 8.8660 - val_acc: 0.4500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3324 - acc: 0.4018 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 1\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.5324 - acc: 0.2634 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3661 - acc: 0.3482 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3087 - acc: 0.3795 - val_loss: 10.6092 - val_acc: 0.3000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3173 - acc: 0.3973 - val_loss: 10.4769 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2332 - acc: 0.4777 - val_loss: 11.2263 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.2906 - acc: 0.4598 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.1368 - acc: 0.5714 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.1077 - acc: 0.5938 - val_loss: 8.0609 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.0060 - acc: 0.5982 - val_loss: 8.7508 - val_acc: 0.4500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.0212 - acc: 0.6027 - val_loss: 5.9708 - val_acc: 0.6000\n",
      "Test accuracy: 0.600000023842\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.4329 - acc: 0.3214 - val_loss: 11.2138 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3060 - acc: 0.3795 - val_loss: 11.2020 - val_acc: 0.3000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2961 - acc: 0.4196 - val_loss: 5.2954 - val_acc: 0.6000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.1940 - acc: 0.5357 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.1346 - acc: 0.5714 - val_loss: 8.8792 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.0354 - acc: 0.6161 - val_loss: 6.1254 - val_acc: 0.5500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.9609 - acc: 0.6295 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.9364 - acc: 0.6384 - val_loss: 6.4472 - val_acc: 0.6000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.8313 - acc: 0.6786 - val_loss: 6.8937 - val_acc: 0.5500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.7219 - acc: 0.7232 - val_loss: 7.2534 - val_acc: 0.5500\n",
      "Test accuracy: 0.550000011921\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 4\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "\t3: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.3711 - acc: 0.3304 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3673 - acc: 0.3036 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3576 - acc: 0.3616 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3675 - acc: 0.3259 - val_loss: 8.5658 - val_acc: 0.4000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.3661 - acc: 0.3170 - val_loss: 10.6521 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3847 - acc: 0.2991 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3675 - acc: 0.3571 - val_loss: 9.2832 - val_acc: 0.3500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3686 - acc: 0.3304 - val_loss: 8.5578 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.3446 - acc: 0.3795 - val_loss: 8.0868 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3226 - acc: 0.3839 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 3\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: CropPercentage (probability=1 percentage_area=0.8 centre=False randomise_percentage_area=False )\n",
      "\t2: Resize (probability=1 width=96 height=96 resample_filter=BICUBIC )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.4706 - acc: 0.2679 - val_loss: 11.2860 - val_acc: 0.3000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3427 - acc: 0.3929 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3836 - acc: 0.2991 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.3407 - acc: 0.3616 - val_loss: 9.3332 - val_acc: 0.3000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.3668 - acc: 0.3214 - val_loss: 9.3586 - val_acc: 0.3500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.3655 - acc: 0.3348 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.3721 - acc: 0.3482 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 1.3309 - acc: 0.4018 - val_loss: 8.8941 - val_acc: 0.4500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 1.3562 - acc: 0.3705 - val_loss: 9.6661 - val_acc: 0.3500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 1.3586 - acc: 0.3259 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " - 1s - loss: 1.4053 - acc: 0.3214 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3190 - acc: 0.4330 - val_loss: 8.0865 - val_acc: 0.5000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3239 - acc: 0.4018 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2948 - acc: 0.4018 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2094 - acc: 0.5045 - val_loss: 8.8655 - val_acc: 0.4500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1779 - acc: 0.5179 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.1050 - acc: 0.5804 - val_loss: 7.2745 - val_acc: 0.5500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.9875 - acc: 0.6429 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.9295 - acc: 0.6562 - val_loss: 6.6840 - val_acc: 0.5500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.9274 - acc: 0.6384 - val_loss: 7.0986 - val_acc: 0.5500\n",
      "Test accuracy: 0.550000011921\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: Shear (probability=0.3 max_shear_left=2 max_shear_right=2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.3889 - acc: 0.3125 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3489 - acc: 0.3750 - val_loss: 9.8143 - val_acc: 0.3500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.3181 - acc: 0.3795 - val_loss: 8.0591 - val_acc: 0.5000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.2771 - acc: 0.4509 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.2067 - acc: 0.5402 - val_loss: 9.2691 - val_acc: 0.3500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1373 - acc: 0.5580 - val_loss: 7.8784 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 1.1151 - acc: 0.5446 - val_loss: 9.6710 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.9969 - acc: 0.6295 - val_loss: 7.7183 - val_acc: 0.5000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.9794 - acc: 0.6161 - val_loss: 7.9228 - val_acc: 0.5000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.8738 - acc: 0.6875 - val_loss: 7.2531 - val_acc: 0.5500\n",
      "Test accuracy: 0.550000011921\n",
      "--------------------------------------------------------------------------------\n",
      "Operations: 2\n",
      "\t0: Flip (probability=0.5 top_bottom_left_right=LEFT_RIGHT )\n",
      "\t1: RandomErasing (probability=0.5 rectangle_area=0.2 )\n",
      "Images: 0\n",
      "Classes: 0\n",
      "\n",
      "You can remove operations using the appropriate index and the remove_operation(index) function.\n",
      "Epoch 1/10\n",
      " - 1s - loss: 1.3890 - acc: 0.3438 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.3357 - acc: 0.4018 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.2702 - acc: 0.4509 - val_loss: 6.8287 - val_acc: 0.4500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 1.1856 - acc: 0.5357 - val_loss: 10.4768 - val_acc: 0.3500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.1668 - acc: 0.5402 - val_loss: 9.6709 - val_acc: 0.4000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.0655 - acc: 0.6116 - val_loss: 7.8892 - val_acc: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.9916 - acc: 0.6116 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.8950 - acc: 0.6652 - val_loss: 7.2531 - val_acc: 0.5500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.8214 - acc: 0.7054 - val_loss: 7.2531 - val_acc: 0.5500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.7766 - acc: 0.7321 - val_loss: 8.0590 - val_acc: 0.5000\n",
      "Test accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "aug_brain.search_opt_augment(model=net.neoaug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute data augmentation with best condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MS125.jpg:   4%|▍         | 7/184 [00:00<00:03, 54.26 Samples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 46 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train/MS/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PD044.jpg:   3%|▎         | 5/184 [00:00<00:05, 32.90 Samples/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 46 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train/PD/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing N080.jpg:  10%|▉         | 18/184 [00:00<00:01, 98.88 Samples/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 79 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train/N/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing PS068.jpg:   3%|▎         | 5/184 [00:00<00:05, 32.35 Samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialised with 75 image(s) found.\n",
      "Output directory set to /home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/train/PS/output."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    }
   ],
   "source": [
    "aug_brain.augment_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hyper parameter of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def data():\n",
      "    train_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/auged' \n",
      "    test_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/validation'\n",
      "    resize = 96 \n",
      "    rescale = 1 \n",
      "    df = load_data.df_fromdir_classed(train_dir)\n",
      "    x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "    df = load_data.df_fromdir_classed(test_dir)\n",
      "    x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "\n",
      "    y_train = to_categorical(y_train)\n",
      "    y_test = to_categorical(y_test)\n",
      "\n",
      "    return x_train, y_train, x_test, y_test\n",
      "\n",
      "         name label        handle  \\\n",
      "0   MS140.jpg    MS  MS/MS140.jpg   \n",
      "1   MS162.jpg    MS  MS/MS162.jpg   \n",
      "2   MS068.jpg    MS  MS/MS068.jpg   \n",
      "3   MS124.jpg    MS  MS/MS124.jpg   \n",
      "4   MS059.jpg    MS  MS/MS059.jpg   \n",
      "5   PD043.jpg    PD  PD/PD043.jpg   \n",
      "6   PD114.jpg    PD  PD/PD114.jpg   \n",
      "7   PD092.jpg    PD  PD/PD092.jpg   \n",
      "8   PD002.jpg    PD  PD/PD002.jpg   \n",
      "9   PD096.jpg    PD  PD/PD096.jpg   \n",
      "10   N160.jpg     N    N/N160.jpg   \n",
      "11   N120.jpg     N    N/N120.jpg   \n",
      "12   N025.jpg     N    N/N025.jpg   \n",
      "13   N035.jpg     N    N/N035.jpg   \n",
      "14   N061.jpg     N    N/N061.jpg   \n",
      "15  PS112.jpg    PS  PS/PS112.jpg   \n",
      "16  PS042.jpg    PS  PS/PS042.jpg   \n",
      "17   PS72.jpg    PS   PS/PS72.jpg   \n",
      "18  PS126.jpg    PS  PS/PS126.jpg   \n",
      "19   PS87.jpg    PS   PS/PS87.jpg   \n",
      "\n",
      "                                                 path  \n",
      "0   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "1   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "2   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "3   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "4   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "5   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "6   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "7   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "8   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "9   /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "10  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "11  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "12  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "13  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "14  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "15  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "16  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "17  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "18  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      "19  /home/fytroo/Tuner/test/standard_datasets/5a66...  \n",
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import subprocess\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import hp\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform, conditional\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import tensorflow as tf\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.datasets import mnist\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import BatchNormalization, Flatten, Dense\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Input, Conv2D, Convolution2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Activation, concatenate, Dropout\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential, Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils import np_utils, to_categorical\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import Augmentor\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import utils\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import load_data\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tuner import net\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas_data import data\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'ch': hp.choice('ch', [16, 32]),\n",
      "        'ch_1': hp.choice('ch_1', [32, 64]),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "  1: \n",
      "  2: train_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/auged' \n",
      "  3: test_dir = '/home/fytroo/Tuner/test/standard_datasets/5a66e0f7b037cb29b22f3106/validation'\n",
      "  4: resize = 96 \n",
      "  5: rescale = 1 \n",
      "  6: df = load_data.df_fromdir_classed(train_dir)\n",
      "  7: x_train, y_train = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      "  8: df = load_data.df_fromdir_classed(test_dir)\n",
      "  9: x_test, y_test = load_data.load_fromdf(df, resize=resize, rescale=rescale)\n",
      " 10: \n",
      " 11: y_train = to_categorical(y_train)\n",
      " 12: y_test = to_categorical(y_test)\n",
      " 13: \n",
      " 14: \n",
      " 15: \n",
      " 16: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     params = eval(open('tmp_params.json', 'r').read())\n",
      "   4: \n",
      "   5:     n_out = params['n_out']\n",
      "   6:     input_shape = tuple(params['input_shape'])\n",
      "   7:     batch_size = params['batch_size']\n",
      "   8:     epochs = params['epochs']\n",
      "   9:     lossfun = params['lossfun']\n",
      "  10:     optimizer = params['optimizer']\n",
      "  11:     metrics = ['accuracy']\n",
      "  12: \n",
      "  13:     inputs = Input(shape=input_shape)\n",
      "  14:     x = inputs\n",
      "  15:     ch = space['ch']\n",
      "  16:     x = Conv2D(ch, (3, 3))(x)\n",
      "  17:     x = Conv2D(ch, (3, 3))(x)\n",
      "  18:     x = Activation('relu')(x)\n",
      "  19:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  20:     ch = space['ch_1']\n",
      "  21:     x = Conv2D(ch, (3, 3))(x)\n",
      "  22:     x = Conv2D(ch, (3, 3))(x)\n",
      "  23:     x = Activation('relu')(x)\n",
      "  24:     x = MaxPooling2D(pool_size=(2, 2))(x)\n",
      "  25:     x = Dropout(0.25)(x)\n",
      "  26:     x = Flatten()(x)\n",
      "  27:     x = Dense(512)(x)\n",
      "  28:     x = Activation('relu')(x)\n",
      "  29:     x = Dropout(0.5)(x)\n",
      "  30:     x = Dense(n_out)(x)\n",
      "  31:     x = Activation('softmax')(x)\n",
      "  32:     model = Model(inputs=inputs, outputs=x)\n",
      "  33:     model.compile(loss=lossfun, optimizer=optimizer, metrics=metrics)\n",
      "  34:     model.fit(\n",
      "  35:         x_train,\n",
      "  36:         y_train,\n",
      "  37:         batch_size=batch_size,\n",
      "  38:         epochs=epochs,\n",
      "  39:         verbose=2,\n",
      "  40:         validation_data=(x_test, y_test))\n",
      "  41:     score, acc = model.evaluate(x_test, y_test, verbose=0)\n",
      "  42:     print('Test accuracy:', acc)\n",
      "  43:     return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n",
      "  44: \n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 12.0460 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 12.1105 - acc: 0.2486 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 12.0448 - acc: 0.2527 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 12.1324 - acc: 0.2473 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 11.9688 - acc: 0.2568 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 11.9995 - acc: 0.2527 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 12.1105 - acc: 0.2486 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 12.0667 - acc: 0.2514 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 12.0448 - acc: 0.2527 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 11.8444 - acc: 0.2649 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 12.1300 - acc: 0.2418 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 12.0862 - acc: 0.2459 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 11.5718 - acc: 0.2758 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 12.1148 - acc: 0.2473 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 12.0010 - acc: 0.2554 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 12.0145 - acc: 0.2541 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 12.0477 - acc: 0.2486 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 12.0897 - acc: 0.2486 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 11.9791 - acc: 0.2568 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 12.1105 - acc: 0.2486 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 11.9867 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 0s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n",
      "Train on 736 samples, validate on 20 samples\n",
      "Epoch 1/10\n",
      " - 1s - loss: 12.0148 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 2/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 3/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 4/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 5/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 6/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 7/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 8/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 9/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Epoch 10/10\n",
      " - 1s - loss: 12.0886 - acc: 0.2500 - val_loss: 12.0886 - val_acc: 0.2500\n",
      "Test accuracy: 0.25\n"
     ]
    }
   ],
   "source": [
    "best_condition, best_model = use_hyperas.exec_hyperas(\n",
    "    aug_brain.augmented_dir,\n",
    "    aug_brain.dataset.validation_dir,\n",
    "    net.simplenet,\n",
    "    batch_size = 32,\n",
    "    epochs=10,\n",
    "    optimizer='adam'\n",
    ")\n",
    "fname = 'simplenet.hdf5'\n",
    "best_model.save(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
